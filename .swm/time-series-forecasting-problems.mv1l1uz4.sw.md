---
title: Time Series Forecasting Problems
---
&nbsp;

 1. **Permutation Invariance and Temporal Information Loss**: Transformers exhibit permutation invariance, meaning their self-attention mechanism is indifferent to data order. Despite positional and temporal embeddings, this can lead to loss of temporal information critical in time series analysis.

 2. **Positional Encoding Limitations**: While positional encoding preserves some order, temporal nuances are often lost due to the permutation-invariance of self-attention in time series transformers.

 3. **Temporal Modeling in Numerical Data**: For time series numerical data lacking semantic elements, it's crucial to model the temporal dynamics among continuous, ordered data points.

 4. **Shuffling Strategies in Transformers**: Two strategies, Shuf (random shuffling of entire sequences) and Half-Ex (exchanging sequence halves), show that transformer performance on time series like Exchange Rate remains stable despite input sequence alterations.

 5. **Time-Dependent Data Properties**: Time series data, typically generated from conditional distributions of past observations, imply weakening inter-observation dependencies over time. Attention mechanisms focused on relative time lags can be more effective, echoing the success of methods like exponential smoothing and ARIMA.

 6. **Positional Encoding Necessity**: Transformers require positional encodings (e.g., Time2Vec) to effectively model time series data.

 7. **Performance Comparison with LSTM**: LSTM may outperform transformers in scenarios with high noise-to-signal ratios, such as in LOB forecasting, due to transformers' sensitivity to noise and tendency to overfit.

 8. **Transformers vs. RNNs/LSTMs**: Despite LSTMs' efficacy in handling consecutive data, they falter with long sequences. Transformers, free from recurrence, leverage attention mechanisms to capture input-output dependencies without order-based bias.

 9. **Time Series Complexity in NLP and CV**: Time series challenges extend beyond order or temporal dependencies, encompassing trend, level, and seasonality crucial for decision-making.

10. **Training Challenges**: Conventional training of transformers for forecasting faces issues like overfitting, data scarcity, and privacy concerns.

11. **Independence in Sample Training**: Machine learning models for time series data must maintain sample independence, often employing K-fold cross-validation. However, this method may not preserve time-ordering and can lead to overfitting due to time-correlated data.

12. **Direct Timestamp Connections in Transformers**: Unlike RNNs, transformers connect directly to all previous timestamps, aiding in longer sequence information propagation but risking input explosion.

13. **Research Interest and Capabilities**: The ability of transformers to capture long-range dependencies has spurred research interest, particularly for time series modeling.

14. **Application in Stock Data Modeling**: Training on historical daily stock data using Time2Vec for time features and transformer encoders can predict output trends effectively.

15. **Anomaly Detection in Time Series**: Transformer architecture aids in time series anomaly detection by modeling temporal dependencies, enhancing detection quality.

&nbsp;

---

&nbsp;

Self-attention is inherently permutation-invariant, i.e., regardless of the order. We argue that even with positional and temporal embeddings, existing Transformer-based methods still suffer from temporal information loss.

- While the positional encoding helps to maintain some ordering information, there remains a temporal information loss due to the permutation-invariant nature of the self-attention component when working with time series transformers
  - Consequently, when analyzing time series numerical data that lacks semantic elements, it is vital to focus on modeling the temporal changes among an ordered set of continuous points.

Two shuffling strategies are presented: Shuf. randomly shuffles the whole input sequences and Half-Ex. exchanges the first half of the input sequence with the second half. Interestingly, compared with the original setting (Ori.) on the Exchange Rate, the performance of all Transformer-based methods does not fluctuate even when the input sequence is randomly shuffled

Time-series data is usually assumed to be generated by a conditional distribution over past observations, with the dependence between observations weakening over time. Therefore, neighboring data points have similar values, and recent tokens should be given a higher weight when measuring their similarity \[13, 14\]. This indicates that attention measured by a relative time lag is more effective than that measured by the ... an assumption further supported by the success of classical exponential smoothing methods and ARIMA model selection methods tending to select small lags.&nbsp;

The Transformer requires a positional encoding, such as \[\[Time2Vec\]\] \[26\] to be added to the input embeddings to model time series.

From Figure 9, we notice that the LSTM outperforms the Transformer model for all forecasting horizons. This could be due to the high noise-to-signal ratio that the LOB contains. The Transformer is more sensitive to noise as it is a more complex neural network which may lead it to overfit the training data even if the training is done using a validation set to reduce the chances of overfitting. We will later see that the Transformerâ€™s encoder performance improves with noise-reducing activation functions and larger training sets.

&nbsp;

**POSITIVE**

Despite specific RNNs, LSTMs show promising performance on consecutive data, have shortcomings in long sequences, such as slowing down too much, and the first sequence data gradually lose their effect.

- To deal with this problem, Transformers come into play. The transformer is a model that gets rid of recurrence and depends on an attention mechanism to extract dependencies between input and output.
  - order of the data does not affect their effect on the output.
- In contrast to the challenges in NLP and CV, time series problems not only add the complexity of order or temporal dependence among input sequences but also consider trend, level, and seasonality information that much of this data is valuable for decision making
  - The conventional training scheme has shown deficiencies regarding model overfitting, data scarcity, and privacy issues when working with transformers for a forecasting task.
  - In order to develop ML models based on time-series data, it is important to maintain independence between the samples, since machine learning generally assumes that the samples are independent.
  - As a result, K-fold cross-validation is commonly used in this scenario \[27\]. In K-fold cross-validation, selected samples serve as validation sets and the remaining samples serve as training sets.
  - The dataset is randomly divided into K equal-size subsets and each is used as a validation set and the remaining subsets as a training set in \[\[k-fold cross-validation|k-fold cross-validation\]\]&nbsp;
  - As a result of applying this algorithm to time series data, there are two problems; first, as time-series data are timeordered, , the assumption would not be preserved if crossvalidation were applied. Second, time-series data tends to be highly correlated with the time axis, consequently, in cases of overfitting, the performance metrics on the validation set are increased


- Transformers, unlike RNNs, have direct connections to all previous timestamps, which allows information to be propagated over longer sequences. In this situation, a new challenge will arise as the model will be directly connected to an exploding input

Moreover, there has been a surge of research interest in transformers for time series modeling by the fact that they can capture long-range dependencies and interactions among the sequential data.

Specifically, we train our model on historical daily stock data where the input integer (day) is used as the time feature for Time2Vec representation \[11\] and a number of transformer encoders are stacked above to predict the output trend

&nbsp;

Specifically, we train our model on historical daily stock data where the input integer (day) is used as the time feature for Time2Vec representation \[11\] and a number of transformer encoders are stacked above to predict the output trend

Transformer based architecture also benefits the time series anomaly detection task with the ability to model temporal dependency, which brings high detection quality

<SwmMeta version="3.0.0" repo-id="Z2l0aHViJTNBJTNBQ1M0Nzk2JTNBJTNBQWxleFQxNDM=" repo-name="CS4796"><sup>Powered by [Swimm](https://app.swimm.io/)</sup></SwmMeta>
