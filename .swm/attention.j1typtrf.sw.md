---
title: Attention
---
Synthesizer \[29\] empirically studies the importance of dot-product interactions, and show that a randomly initialized, learnable attention mechanisms with or without token-token dependencies can achieve competitive performance with vanilla self-attention on various NLP tasks.

Used unparameterized Gaussian distribution to replace the original attention scores, concluding that the attention distribution should focus on a certain local window and can achieve comparable performance.

&nbsp;

![](https://firebasestorage.googleapis.com/v0/b/swimmio-content/o/repositories%2FZ2l0aHViJTNBJTNBQ1M0Nzk2JTNBJTNBQWxleFQxNDM%3D%2Feb0f3515-da63-4ab2-bf54-7aa3c9bbb186.png?alt=media&token=be6a2daf-d07b-4555-a3de-1aabc5758a37)

<SwmMeta version="3.0.0" repo-id="Z2l0aHViJTNBJTNBQ1M0Nzk2JTNBJTNBQWxleFQxNDM=" repo-name="CS4796"><sup>Powered by [Swimm](https://app.swimm.io/)</sup></SwmMeta>
